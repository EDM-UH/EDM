{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6.2 - Testing and Validation\n",
    "In the previous notebook, we fit, or *trained* an estimator using a feature matrix *X* and a label vector *y*. We then tested the accuracy of this estimator by comparing its predictions based on *X* with *y*. This is called the estimator's **training accuracy**. Training accuracy is an important, but deceptive metric. Estimators may sometimes overfit their data, resulting in very high training performance only to do poorly on new data. The notion that a model should perform well on previously unseen data is called **generalizability**.\n",
    "### We test for generalizability by splitting our data into a *training set* and *test set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We take a shortcut this time in creating our X's and y's\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the train_test_split() function returns a 4-tuple\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's see how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.975\n",
      "Testing accuracy: 0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model's training performance\n",
    "y_train_pred = lr.predict(X_train)\n",
    "print('Training accuracy:', accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# evaluate model's test performance\n",
    "y_test_pred = lr.predict(X_test)\n",
    "print('Testing accuracy:', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Your turn\n",
    "Where as predicting the species in the iris dataset was a **classification** problem where the prediction can only be from a set of distinct categories, the following is a regression problem where the prediction is a continuous number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(1000,3) * 100\n",
    "y = (X[:,0] + 2 * X[:,1] + X[:,2]) + 10 * np.random.random(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Use Ridge, it is located in sklearn.linear_model\n",
    "- In a regression problem, accuracy does not apply. Use mean_squared_error instead in sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant estimator\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create your training and testing data from X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
    "\n",
    "# Instantiate Ridge estimator object\n",
    "\n",
    "\n",
    "# fit model to training data\n",
    "\n",
    "\n",
    "# evaluate model's training performance\n",
    "\n",
    "\n",
    "# evaluate model's test performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Let's go back to iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the train_test_split() function returns a 4-tuple\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# One of the commandments of predictive analysis is you never change your model after seeing the testing performance.\n",
    "Why? Because changing model parameters to get the testing performance higher introduces the possibility of overfitting to your test data. You will never be able to test the generalizability of your model. In order to test the performance of a model without touching the test data, we use **cross-validation**. Instead of splitting data into two groups, training and test, we split it into three: training, validation, and test. Testing on the validation set allows us to assess a model's performance and save the test data for a final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.929004329004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "svm = SVC(C = 2000)\n",
    "\n",
    "# fit model to training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model's training performance\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print('Training accuracy:', accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# evaluate model's validation performance\n",
    "print('Validation accuracy:', cross_val_score(estimator = svm, X = X_train, y = y_train, cv = 5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.921052631579\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = svm.predict(X_test)\n",
    "print('Testing accuracy:', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
